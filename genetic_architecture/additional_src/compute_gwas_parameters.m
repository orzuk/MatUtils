% Compute several statistical parameters from GWAS results
%
% Input:
% gwas_data - a structure with all parameters on each gwas
% field - by which field should we treat similar snps together (default is PUBMEDIT, look by study)
%
% Output:
% lambda_s - sibling relative risk
% lambda_mz - monzigotic twins relative risk (assume a multiplicative model!)
% lambda_mz_add - mz risk given additive model
% h_add - additive heritability
% V_add - additive variance
% h - total heritability
% V - total variance
% unique_inds - indices of disease
% num_loci - number of loci found for each disease
% study_max_penetrance - maximal disease probability for the worst genotype
% h_liab - heritability on liability threshold
% prevalence - disease prevalence
% binary_inds - indices of binary traits
% trait_name - name of each trait
% snp_power_vec - detection power for each individual SNP
% snp_power_empirical_vec - detection power for each individual SNP (computed empirically)
% snp_lambda_s - associated sibling relative risk for each SNP
% snp_h_liab - fraction of variance explained on liability/QTL scale for each SNP
% effective_sample_size - non-centrality effect of sample size
% alpha_vec - the p-value cutoffs used to achieve the power
% beta_discovery_grid - boundary of discoveries for each trait
% beta_discovery_inv_grid - boundary of discoveries for each trait (inverse effect size)
%
function data_params = compute_gwas_parameters(gwas_data, field)

AssignGeneticArchitectureConstants;

new_compute_power = 1; % use new correction
if(~exist('field', 'var')) % by which field do we unite different associations
    field = 'PUBMEDID';
end
eval_str = ['[unique_id unique_inds J] = unique(gwas_data.' ...
    field ', ''first'');']; % take the first row in each study
eval(eval_str);
% [unique_pubmed_id unique_inds J] = unique(gwas_data.PUBMEDID); %   PubMedID); % divide to studies

num_studies = length(unique_id);
num_snps = length(gwas_data.SNPs);

lambda_s_vec = cell(num_studies,1);
lambda_s = zeros(num_studies,1);
lambda_mz = zeros(num_studies,1);
h_add = zeros(num_studies,1);
h = zeros(num_studies,1);
V_add = cell(num_studies,1);
V = zeros(num_studies,1);
num_loci = zeros(num_studies,1);

snp_lambda_s = zeros(num_snps,1);
snp_h_liab = zeros(num_snps,1);
snp_h_liab_min = zeros(num_snps,1);
snp_h_liab_max = zeros(num_snps,1);
study_max_penetrance = zeros(num_studies,1);
% prevalence = 0.05; % temporary until we know true prevalence ...

binary_inds = find( gwas_data.trait_type_num == 0); % indices of SNPs
disease_binary_inds = find( gwas_data.trait_type_num(unique_inds) == 0); % indices of disease! (not SNPs)

quant_inds = setdiff(1:num_snps, binary_inds);
compute_power_flag = 1;
alpha_vec = 5.*10.^[-9:0.1:-5]; % Set a strict genome-wide threshold
num_cutoffs = length(alpha_vec);
snp_power_vec = zeros(num_snps,num_cutoffs);
snp_power_empirical_vec = snp_power_vec; snp_non_centrality_vec = snp_power_vec; % add different power parameters (debugging!)
if(compute_power_flag) % Compute power of each marker
    
    iters = 10; % set iterations for power calculations (takes lots of time)
    p_z_x_marginal = zeros(num_snps,4);
    p_z_x_marginal(binary_inds,:) = genetic_relative_risk_to_p_z_x_marginal( ... % use REPLICATION effect size !!!
        gwas_data.RAF(binary_inds), gwas_data.OR(binary_inds,2), ...
        gwas_data.Prevalence(binary_inds));
    test_type = 'armitage'; % 'single-locus';
    
    snp_effect_std_vec = zeros(num_snps,1); % compute std.s. and confidence intervals for effect sizes
    snp_effect_min_vec = zeros(num_snps,1);
    snp_effect_max_vec = zeros(num_snps,1);
    [snp_effect_std_vec(binary_inds) snp_effect_min_vec(binary_inds) ...
        snp_effect_max_vec(binary_inds)] = ...  % Compute confidence intervals for all effect sizes for binary traits
        genetic_relative_risk_to_confidence_interval(gwas_data.OR(binary_inds,2), ...
        gwas_data.RAF(binary_inds), ...
        gwas_data.Prevalence(binary_inds), ...
        gwas_data.replication_num_cases(binary_inds),  gwas_data.replication_num_controls(binary_inds));
    
    %    for j=[2] % compute confidence only for replication (?)
    [snp_effect_std_vec(quant_inds) snp_effect_min_vec(quant_inds) ...
        snp_effect_max_vec(quant_inds)] = ...  % Compute confidence intervals for all effect sizes for quant traits
        beta_to_confidence(gwas_data.Beta(quant_inds,2), gwas_data.RAF(quant_inds), ...
        gwas_data.replication_num_cases(quant_inds));
    %    end
    N=10^4; x_grid = 0.5.*(1:N)./N; % set effective population size and MAF grid
    s_input = 0; % try neutral
    alpha = 5*10^(-8); % alpha_vec(alpha_power_ind);
    
    for i=1:length(binary_inds) % compute power one by one for binary traits
        %        if(mod(i,50) == 0)
        compute_power_binary_snp = i
        %        end
        cur_num_cases = gwas_data.discovery_num_cases(binary_inds(i));
        cur_num_controls = gwas_data.discovery_num_controls(binary_inds(i));
        
        
        switch gwas_data.Trait{binary_inds(i)} % special correction: replication add sample size
            case 'Breast Cancer' % special correction
                %                 cur_num_cases = gwas_data.replication_num_cases(binary_inds(i));
                %                 cur_num_controls = gwas_data.replication_num_controls(binary_inds(i));
                
                
                % % % % % % %                 maximal_OR = max(gwas_data.Power_OR(binary_inds(i)), ...
                % % % % % % %                     gwas_data.OR(binary_inds(i),2)); % be conservative - take max power
                % % % % % % %                 minimal_OR = min(gwas_data.Power_OR(binary_inds(i)), ...
                % % % % % % %                     gwas_data.OR(binary_inds(i),2)); % be conservative - take min power
                % % % % % % %                 [~, max_ind] = max([abs(maximal_OR-1) abs(minimal_OR-1)]);
                % % % % % % %                 if(max_ind == 2) % flip to minimal
                % % % % % % %                     maximal_OR = minimal_OR;
                % % % % % % %                 end
                % % % % % % %                 p_z_x_marginal(binary_inds(i),:) = ...
                % % % % % % %                     genetic_relative_risk_to_p_z_x_marginal( ...
                % % % % % % %                     gwas_data.RAF(binary_inds(i)), maximal_OR, ...
                % % % % % % %                     gwas_data.Prevalence(binary_inds(i)));
                
            case {'Crohn', 'Crohns', 'Crohn''s disease'} % special case: Crohn's - take combined sample size! why??
                cur_num_cases = 6333+15694;
                cur_num_controls = 15056+14026;
                
        end
%         new_compute_power = 0;
%         if(new_compute_power) % new power computation: takes a long time! (but worth it!)
%             test_type = 'armitage'; test_stat = 'chi-square';
%             [beta_discovery_grid beta_discovery_inv_grid] = ...
%                 compute_discovery_boundary(x_grid, gwas_data.Prevalence(binary_inds(i)), ...
%                 cur_num_cases, cur_num_controls, alpha(1), ...
%                 test_type, test_stat); % Determine discovery boundry
%             
%             
%             [selection_coefficient{binary_inds(i)} selection_slope{binary_inds(i)} ...
%                 LogLikelihood{binary_inds(i)} ...
%                 VarExplained{binary_inds(i)} VarExplainedCorrected{binary_inds(i)} ...
%                 V_corrected_noisy V_corrected_strings{i} pow_integral{i} power_log_vec{i}] = ... %                ... V_corrected_noisy{i} V_corrected_strings{i} pow_integral{i} power_log_vec{i}] = ...
%                 maximize_gwas_observed_allele_freq_and_effect_size_likelihood( ...
%                 gwas_data.RAF(binary_inds(i)), ...
%                 [gwas_data.OR(binary_inds(i),2) gwas_data.Prevalence(binary_inds(i))], ...
%                 [gwas_data.OR(binary_inds(i),2) gwas_data.Prevalence(binary_inds(i))],...
%                 x_grid, beta_discovery_grid, ...
%                 N, gwas_data.Prevalence(binary_inds(i)), ...
%                 cur_num_cases, max(cur_num_controls,1), alpha, 1, ...
%                 s_input, 'replication', gwas_data.trait_type{binary_inds(i)}); % find optimal s, (beta) and compute correction
%             new_power_vec_corrections(binary_inds(binary_inds(i)),:) = exp(power_log_vec{i});% New corrections: Time consuming !!!!!
%         end
        
        
        [snp_power_vec(binary_inds(i),:), ~, ~, snp_non_centrality_vec(binary_inds(i),:)] = ...
            compute_association_power(p_z_x_marginal(binary_inds(i),:), ...
            cur_num_cases, cur_num_controls, alpha_vec, iters, ...
            test_type, 'chi-square-analytic', 'case-control'); % compute power to detect each locus analytically
        snp_power_empirical_vec(binary_inds(i),:) = ... %  chi_p_vals_vec chi_stat_vec]  = ...
            compute_association_power(p_z_x_marginal(binary_inds(i),:), ...
            cur_num_cases, cur_num_controls, alpha_vec, iters, ...
            test_type, 'chi-square', 'case-control'); % compute power to detect each locus empirically
        
    end
    num_quant_inds = length(quant_inds);
    
    p_z_x_marginal(quant_inds,:) = QTL_params_to_p_mat(gwas_data.RAF(quant_inds), ...
        gwas_data.Beta(quant_inds,2), ones(length(quant_inds),1)); % use REPLICATION effect sizes
    
    %     %    p_z_x_marginal = zeros(num_quant_inds,4);
    %     p_z_x_marginal(quant_inds,1) = gwas_data.RAF(quant_inds); % Set risk-allele-frequency
    %     p_z_x_marginal(quant_inds,2) = gwas_data.Beta(quant_inds); % set effect size (beta, the regression coefficient)
    %     p_z_x_marginal(quant_inds,4) = 1; % gwas_data.RAF(quant_inds); % Set trait's variance (default is one
    
    for i=1:length(quant_inds) % compute power one by one for QTL
        %        if(mod(i,50) == 0)
        compute_power_quant_snp = i
        %        end
        
        switch gwas_data.Trait{quant_inds(i)} % special correction: replication add sample size
            case 'Height' % use replication set size. Why???
                cur_num_cases = 183727;
            otherwise
                cur_num_cases = gwas_data.discovery_num_cases(quant_inds(i));
        end
        [snp_power_vec(quant_inds(i),:), ~, ~, snp_non_centrality_vec(quant_inds(i),:)] = ...
            compute_association_power(p_z_x_marginal(quant_inds(i),:), ...
            cur_num_cases, [], alpha_vec, iters, ...
            'single-locus', 'chi-square-QTL-analytic', 'population'); % compute power to detect each locus
        snp_power_empirical_vec(quant_inds(i),:) = ...
            compute_association_power(p_z_x_marginal(quant_inds(i),:), ...
            cur_num_cases, [], alpha_vec, iters/10, ... % use fewer iters (quant is very slow ..)
            'single-locus', 'chi-square-QTL', 'population'); % compute power to detect each locus
        
%         new_compute_power = 0;
%         if(new_compute_power) % new power computation: takes a long time! (but worth it!)
%             N=10^4; x_grid = 0.5.*(1:N)./N; % set effective population size and MAF grid
%             s_input = 0; % try neutral
%             alpha = 5*10^(-8); % alpha_vec(alpha_power_ind);
%             test_type = 'single-locus'; test_stat = 'chi-square-QTL';
%             [beta_discovery_grid beta_discovery_inv_grid] = ...
%                 compute_discovery_boundary(x_grid, [], ...
%                 cur_num_cases, cur_num_controls, alpha(1), ...
%                 test_type, test_stat); % Determine discovery boundry
%             [selection_coefficient{quant_inds(i)} selection_slope{quant_inds(i)} ...
%                 LogLikelihood{quant_inds(i)} ...
%                 VarExplained{quant_inds(i)} VarExplainedCorrected{quant_inds(i)} ...
%                 V_corrected_noisy V_corrected_strings{i} pow_integral{i} power_log_vec{i}] = ... %                ... V_corrected_noisy{i} V_corrected_strings{i} pow_integral{i} power_log_vec{i}] = ...
%                 maximize_gwas_observed_allele_freq_and_effect_size_likelihood( ...
%                 gwas_data.RAF(quant_inds(i)), ...
%                 gwas_data.Beta(quant_inds(i),2), ...
%                 gwas_data.Beta(quant_inds(i),2),...
%                 x_grid, beta_discovery_grid, ...
%                 N, gwas_data.Prevalence(quant_inds(i)), ...
%                 cur_num_cases, [], alpha, 1, ...
%                 s_input, 'replication', gwas_data.trait_type{quant_inds(i)}); % find optimal s, (beta) and compute correction
%             new_power_vec_corrections(binary_inds(i),:) = exp(power_log_vec{i});% New corrections: Time consuming !!!!!
%         end
        
        
    end % loop on quant inds
    
    
    for i=1:num_studies % loop on all different traits and compute new sophisticated power correction
        study_inds = find(J == i); num_loci(i) = length(study_inds);
        switch gwas_data.Trait{study_inds(1)}
            case special_traits % {'Height', 'Type 2 diabetes'}
                do_special_triat = 99912313123
            otherwise
                continue; % skip this trait (plot only for special (to save time ... ))
        end
        do_i = i
        % trait_inds = strmatch(disease_names{i}, gwas_data.Trait); % get all indices of current trait
        %study_RAF = mat_into_vec([gwas_data.RAF(study_inds) ...
        %    gwas_data.RAF(study_inds)]'); % duplicate each allele
        compute_stats_disease_name = gwas_data.Trait{study_inds(1)}
        cur_num_cases = gwas_data.discovery_num_cases(study_inds(1));
        cur_num_controls = gwas_data.discovery_num_controls(study_inds(1));
        
        switch gwas_data.trait_type{study_inds(1)} % set test type
            case {'Binary', 'binary'}
                test_type = 'armitage'; test_stat = 'chi-square';
                study_effect_sizes = [gwas_data.OR(study_inds,2) gwas_data.Prevalence(study_inds)];
            case {'QTL', 'Quantitative'}
                test_type = 'single-locus'; test_stat = 'chi-square-QTL';
                study_effect_sizes = gwas_data.Beta(study_inds,2);
                cur_num_controls = [];
        end
        [beta_discovery_grid{i} beta_discovery_inv_grid{i}] = ...
            compute_discovery_boundary(x_grid, gwas_data.Prevalence(study_inds(1)), ...
            cur_num_cases, cur_num_controls, alpha(1), ...
            test_type, test_stat); % Determine discovery boundry
        [selection_coefficient{i} selection_slope{i} ...
            LogLikelihood{i} ...
            VarExplained{i} VarExplainedCorrected{i} ...
            V_corrected_noisy V_corrected_strings{i} pow_integral{i} power_log_vec{i}] = ... %                ... V_corrected_noisy{i} V_corrected_strings{i} pow_integral{i} power_log_vec{i}] = ...
            maximize_gwas_observed_allele_freq_and_effect_size_likelihood( ...
            gwas_data.RAF(study_inds), ...
            study_effect_sizes, ...
            study_effect_sizes,...
            x_grid, beta_discovery_grid{i}, ...
            N, gwas_data.Prevalence(study_inds(1)), ...
            cur_num_cases, cur_num_controls, alpha, 1, ...
            s_input, 'replication', gwas_data.trait_type{study_inds(1)}); % find optimal s, (beta) and compute correction
        
        new_power_vec_corrections(study_inds,:) = vec2column(exp(power_log_vec{i}));% New corrections: Time consuming !!!!!
        
    end % loop on studies and compute power for each ! (note! need to make sure we record SNP-specific parameters!)
    
    effective_sample_size = zeros(num_snps,1);
    for i=1:num_snps % Compute general ncp power for each snp (?)
        effective_sample_size(i) = sample_size_to_power_effect_index( ...
            gwas_data.discovery_num_cases(i), gwas_data.discovery_num_controls(i), ...
            gwas_data.trait_type{i});
    end
    
end % if compute power



for i=1:num_studies % add additional parameters (what???)
    compute_stats_disease = i
    study_inds = find(J == i); num_loci(i) = length(study_inds);
    study_RAF = mat_into_vec([gwas_data.RAF(study_inds) ...
        gwas_data.RAF(study_inds)]'); % duplicate each allele
    compute_stats_disease_name = gwas_data.Trait{study_inds(1)}
    switch gwas_data.trait_type{study_inds(1)} % compute var. explained for REPLICATION effect sizes
        case {'binary', 'Binary'}
            %    i_study_is = i
            study_GRR = mat_into_vec([gwas_data.GRR(study_inds,2) ...
                gwas_data.GRR(study_inds,2)]'); % duplicate each allele
            study_log_weight = log( gwas_data.Prevalence(study_inds(1))) - ...
                sum( log(1-study_RAF + study_RAF .* study_GRR));
            study_max_penetrance(i) = exp(study_log_weight) * prod(study_GRR);
            [lambda_s_vec{i} lambda_s(i) lambda_mz_add(i) h_add(i) V_add{i} ...
                lambda_s_mult(i) lambda_mz(i) h(i) V(i) ...
                sub_freq_mat h_liab(i) h_liab_vec{i}] = ...
                genetic_relative_risk_to_heritability(study_RAF, ...
                study_GRR, gwas_data.Prevalence(study_inds(1)));
            prevalence(i) = gwas_data.Prevalence(study_inds(1)); % copy prevalence
            snp_lambda_s(study_inds) = lambda_s_vec{i}(1:2:end).^2; % update effect size for each SNP on sibling relative risk
            snp_h_liab(study_inds) = h_liab_vec{i}(1:2:end) * 2; % update effect size for each SNP on variance explained
            
            study_GRR_max = mat_into_vec([snp_effect_max_vec(study_inds) ...
                snp_effect_max_vec(study_inds)]'); % duplicate each allele
            study_GRR_min = mat_into_vec([snp_effect_min_vec(study_inds) ...
                snp_effect_min_vec(study_inds)]'); % duplicate each allele
            [~, ~, ~,  ~,  ~, ~, ~, ~, ~, ...
                ~, ~, h_liab_min_vec{i}] = ...
                genetic_relative_risk_to_heritability(study_RAF, ...
                study_GRR_min, gwas_data.Prevalence(study_inds(1)));
            snp_h_liab_min(study_inds) = h_liab_min_vec{i}(1:2:end) * 2;
            [~, ~, ~,  ~,  ~, ~, ~, ~, ~, ...
                ~, ~, h_liab_max_vec{i}] = ...
                genetic_relative_risk_to_heritability(study_RAF, ...
                study_GRR_max, gwas_data.Prevalence(study_inds(1)));
            snp_h_liab_max(study_inds) = h_liab_max_vec{i}(1:2:end) * 2;
            % Compute confiedence on var. explained
            
        case {'QTL', 'Quantitative'} % apply simple formula:
            study_VAR = 1; % Important! assume a quantitative trait is already normalized !!
            snp_h_liab(study_inds) = beta_to_variance_explained(gwas_data.Beta(study_inds,2), ...
                gwas_data.RAF(study_inds), study_VAR, 'diploid');
            snp_h_liab_min(study_inds) = beta_to_variance_explained(snp_effect_min_vec(study_inds), ...
                gwas_data.RAF(study_inds), study_VAR, 'diploid');
            snp_h_liab_max(study_inds) = beta_to_variance_explained(snp_effect_max_vec(study_inds), ...
                gwas_data.RAF(study_inds), study_VAR, 'diploid');
            
    end
    trait_name{i} = gwas_data.Trait{study_inds(1)}; % copy name
    
    if(h(i) > 1)
        bad_h_xxx = 124124
    end
    
    if(exist('study_n_cases', 'var')) % need to obtain #cases
        [total_lambda_s(i) total_lambda_s_adjusted(i) ...
            total_h_liab(i) total_h_liab_adjusted(i)] = ...
            adjust_by_power(study_RAF, study_GRR, gwas_data.Prevalence(study_inds(1)), ...
            study_n_cases, study_n_controls); % compute heritability parameters assming more loci were found
    end
end % loop on studies


%     [data_params.lambda_s data_params.lambda_mz data_params.lambda_mz_add data_params.h_add data_params.V_add ...
%         data_params.h data_params.V data_params.unique_inds data_params.num_loci ...
%         data_params.max_penetrance data_params.h_liab data_params.Prevalence ...
%         data_params.binary_inds data_params.trait_name ...
%         data_params.Power data_params.Power_empirical data_params.non_centrality ...
%         data_params.std data_params.OR_min_confidence data_params.OR_max_confidence ... % New: add confidence parameters
%         data_params.snp_lambda_s data_params.snp_h_liab data_params.snp_h_liab_min data_params.snp_h_liab_max ...
%         data_params.effective_sample_size data_params.alpha_vec ...
%         data_params.beta_discovery_grid, data_params.beta_discovery_inv_grid] = ...

data_params = struct('lambda_s', lambda_s, ...
    'lambda_mz', lambda_mz, ...
    'lambda_mz_add', lambda_mz_add, ...
    'h_add', h_add, ...
    'h', h, ...
    'V', V, ...
    'unique_inds', unique_inds, ...
    'num_loci', num_loci, ...
    'max_penetrance', study_max_penetrance, ...
    'h_liab', h_liab, ...
    'Prevalence', prevalence, ...
    'binary_inds', binary_inds, ...
    'Power', snp_power_vec, ...
    'Power_empirical', snp_power_empirical_vec, ...
    'non_centrality', snp_non_centrality_vec,...    % get snp-specific power parameters
    'std', snp_effect_std_vec, ...
    'OR_min_confidence', snp_effect_min_vec, ...
    'OR_max_confidence', snp_effect_max_vec, ...
    'snp_lambda_s', snp_lambda_s, ...
    'snp_h_liab', snp_h_liab, ...
    'snp_h_liab_min', snp_h_liab_min, ...
    'snp_h_liab_max', snp_h_liab_max, ...
    'effective_sample_size', effective_sample_size, ...
    'alpha_vec', alpha_vec, ...
    'x_grid', x_grid);
% Add all the cell arrays
data_params.beta_discovery_grid = beta_discovery_grid;
data_params.beta_discovery_inv_grid= beta_discovery_inv_grid;
data_params.VarExplained= VarExplained;
data_params.VarExplainedCorrected= VarExplainedCorrected;
data_params.V_corrected_strings= V_corrected_strings;
data_params.pow_integral= pow_integral;
data_params.trait_name= trait_name;
data_params.V_add= V_add;
